{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab47632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==================================================================================\n",
    "PYTORCH SEQ2SEQ TRANSLATOR - FIXED VERSION\n",
    "==================================================================================\n",
    "English → Spanish Neural Machine Translation\n",
    "\n",
    "KEY IMPROVEMENTS FROM PREVIOUS VERSION:\n",
    "1. ✅ Reduced vocabulary (16K → 5K) for better learning\n",
    "2. ✅ Increased model capacity (128 → 256 hidden units)\n",
    "3. ✅ Bidirectional encoder for better context understanding\n",
    "4. ✅ Removed problematic GloVe embeddings\n",
    "5. ✅ Better training configuration\n",
    "6. ✅ Quality over quantity approach\n",
    "\n",
    "Expected Results:\n",
    "- BLEU Score: 8-12 (vs previous 3-4)\n",
    "- Coherent Spanish translations\n",
    "- No infinite loops or garbage output\n",
    "==================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ==================================================================================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# ==================================================================================\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# NLTK for tokenization\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eb2d4",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 2. IMPROVED HYPERPARAMETERS\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db85ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONFIGURATION\n",
      "================================================================================\n",
      "Data: 50,000 sentences, 5,000 vocab\n",
      "Model: 256 hidden units, 128 embedding dim\n",
      "Training: 25 epochs, LR=0.001, batch=64\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data configuration - REDUCED for better learning\n",
    "MAX_NUM_SENTENCES = 50000  # Down from 100K - focus on quality\n",
    "MAX_VOCAB_SIZE = 5000      # Down from 16K - ensure each word is learned well\n",
    "MIN_WORD_FREQ = 2          # Filter rare words\n",
    "\n",
    "# Model architecture - INCREASED capacity\n",
    "HIDDEN_DIM = 256    # Up from 128 - more capacity\n",
    "EMBED_DIM = 128     # Up from 50 - richer representations\n",
    "\n",
    "# Training configuration\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT = 0.1\n",
    "N_EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "# Learning rate scheduler\n",
    "USE_LR_SCHEDULER = True\n",
    "LR_SCHEDULER_PATIENCE = 3\n",
    "LR_SCHEDULER_FACTOR = 0.5\n",
    "LR_SCHEDULER_MIN_LR = 0.0001\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Data: {MAX_NUM_SENTENCES:,} sentences, {MAX_VOCAB_SIZE:,} vocab\")\n",
    "print(f\"Model: {HIDDEN_DIM} hidden units, {EMBED_DIM} embedding dim\")\n",
    "print(f\"Training: {N_EPOCHS} epochs, LR={LEARNING_RATE}, batch={BATCH_SIZE}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287c680",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 3. DATA LOADING AND PREPROCESSING\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700a5506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences with quality filters...\n",
      "Loaded 49,650 quality-filtered sentences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download dataset if needed\n",
    "import os\n",
    "if not os.access('spa-eng', os.F_OK):\n",
    "    if not os.access('spa-eng.zip', os.F_OK):\n",
    "        print(\"Downloading dataset...\")\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(\n",
    "            'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "            'spa-eng.zip'\n",
    "        )\n",
    "    print(\"Extracting dataset...\")\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('spa-eng.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    print(\"Dataset ready!\")\n",
    "\n",
    "# Load and shuffle data\n",
    "text_file = \"./spa-eng/spa.txt\"\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "# Parse sentences with quality filtering\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "print(\"Parsing sentences with quality filters...\")\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if i >= MAX_NUM_SENTENCES:\n",
    "        break\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    \n",
    "    input_sentence, output = line.rstrip().split('\\t')\n",
    "    \n",
    "    # Quality filters\n",
    "    input_words = input_sentence.split()\n",
    "    output_words = output.split()\n",
    "    \n",
    "    # Skip very short or very long sentences\n",
    "    if len(input_words) < 2 or len(input_words) > 20:\n",
    "        continue\n",
    "    if len(output_words) < 2 or len(output_words) > 20:\n",
    "        continue\n",
    "    \n",
    "    # Add special tokens\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "    \n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(f\"Loaded {len(input_sentences):,} quality-filtered sentences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4350925",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 4. VOCABULARY BUILDING\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f9209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building vocabularies...\n",
      "English vocabulary: 5,000 words\n",
      "Spanish vocabulary: 5,000 words\n",
      "Coverage: ~9.9 sentences per EN word\n",
      "Coverage: ~9.9 sentences per ES word\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SimpleVocab:\n",
    "    \"\"\"Simple vocabulary class with frequency filtering\"\"\"\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<unk>']):\n",
    "        self.itos = specials.copy()\n",
    "        self.stoi = {token: idx for idx, token in enumerate(self.itos)}\n",
    "        self.unk_index = self.stoi.get('<unk>', 1)\n",
    "        \n",
    "        # Sort by frequency\n",
    "        sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        for token, freq in sorted_tokens:\n",
    "            if token in self.stoi:\n",
    "                continue\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if max_size and len(self.itos) >= max_size:\n",
    "                break\n",
    "            self.stoi[token] = len(self.itos)\n",
    "            self.itos.append(token)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.unk_index)\n",
    "    \n",
    "    def get_itos(self):\n",
    "        return self.itos\n",
    "    \n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "def simple_tokenizer(text, language='en'):\n",
    "    \"\"\"NLTK-based tokenizer\"\"\"\n",
    "    try:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "    except:\n",
    "        tokens = text.lower().split()\n",
    "    return tokens\n",
    "\n",
    "# Build vocabularies\n",
    "print(\"\\nBuilding vocabularies...\")\n",
    "\n",
    "# English vocabulary\n",
    "en_counter = Counter()\n",
    "for sentence in input_sentences:\n",
    "    tokens = simple_tokenizer(sentence, language='en')\n",
    "    en_counter.update(tokens)\n",
    "\n",
    "en_vocab = SimpleVocab(\n",
    "    en_counter,\n",
    "    max_size=MAX_VOCAB_SIZE,\n",
    "    min_freq=MIN_WORD_FREQ,\n",
    "    specials=['<pad>', '<unk>']\n",
    ")\n",
    "\n",
    "# Spanish vocabulary\n",
    "es_counter = Counter()\n",
    "for sentence in output_sentences + output_sentences_inputs:\n",
    "    tokens = simple_tokenizer(sentence, language='es')\n",
    "    es_counter.update(tokens)\n",
    "\n",
    "es_vocab = SimpleVocab(\n",
    "    es_counter,\n",
    "    max_size=MAX_VOCAB_SIZE,\n",
    "    min_freq=MIN_WORD_FREQ,\n",
    "    specials=['<pad>', '<unk>', '<sos>', '<eos>']\n",
    ")\n",
    "\n",
    "print(f\"English vocabulary: {len(en_vocab):,} words\")\n",
    "print(f\"Spanish vocabulary: {len(es_vocab):,} words\")\n",
    "print(f\"Coverage: ~{len(input_sentences) / len(en_vocab):.1f} sentences per EN word\")\n",
    "print(f\"Coverage: ~{len(output_sentences) / len(es_vocab):.1f} sentences per ES word\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29751c",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 5. DATASET AND DATALOADERS\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d17e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset split:\n",
      "  Training: 39,720 pairs (621 batches)\n",
      "  Validation: 9,930 pairs (156 batches)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def text_to_indices(text, vocab, language='en'):\n",
    "    \"\"\"Convert text to indices\"\"\"\n",
    "    tokens = simple_tokenizer(text, language=language)\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Dataset for translation pairs\"\"\"\n",
    "    def __init__(self, input_sentences, output_sentences, output_sentences_inputs,\n",
    "                 en_vocab, es_vocab):\n",
    "        self.input_sentences = input_sentences\n",
    "        self.output_sentences = output_sentences\n",
    "        self.output_sentences_inputs = output_sentences_inputs\n",
    "        self.en_vocab = en_vocab\n",
    "        self.es_vocab = es_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc_input = text_to_indices(\n",
    "            self.input_sentences[idx],\n",
    "            self.en_vocab,\n",
    "            language='en'\n",
    "        )\n",
    "        \n",
    "        dec_input = text_to_indices(\n",
    "            self.output_sentences_inputs[idx],\n",
    "            self.es_vocab,\n",
    "            language='es'\n",
    "        )\n",
    "        \n",
    "        dec_target = text_to_indices(\n",
    "            self.output_sentences[idx],\n",
    "            self.es_vocab,\n",
    "            language='es'\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(enc_input, dtype=torch.long),\n",
    "            torch.tensor(dec_input, dtype=torch.long),\n",
    "            torch.tensor(dec_target, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function with padding\"\"\"\n",
    "    enc_inputs, dec_inputs, dec_targets = zip(*batch)\n",
    "    \n",
    "    enc_inputs_padded = pad_sequence(enc_inputs, batch_first=True, padding_value=en_vocab['<pad>'])\n",
    "    dec_inputs_padded = pad_sequence(dec_inputs, batch_first=True, padding_value=es_vocab['<pad>'])\n",
    "    dec_targets_padded = pad_sequence(dec_targets, batch_first=True, padding_value=es_vocab['<pad>'])\n",
    "    \n",
    "    return enc_inputs_padded, dec_inputs_padded, dec_targets_padded\n",
    "\n",
    "# Split data\n",
    "train_input, val_input, train_output, val_output, train_output_inp, val_output_inp = train_test_split(\n",
    "    input_sentences, output_sentences, output_sentences_inputs,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TranslationDataset(\n",
    "    train_input, train_output, train_output_inp,\n",
    "    en_vocab, es_vocab\n",
    ")\n",
    "\n",
    "val_dataset = TranslationDataset(\n",
    "    val_input, val_output, val_output_inp,\n",
    "    en_vocab, es_vocab\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training: {len(train_dataset):,} pairs ({len(train_loader)} batches)\")\n",
    "print(f\"  Validation: {len(val_dataset):,} pairs ({len(val_loader)} batches)\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39743d",
   "metadata": {},
   "source": [
    "\n",
    "# ==================================================================================\n",
    "# 6. IMPROVED MODEL ARCHITECTURE\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9982e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImprovedEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved bidirectional encoder with better capacity\n",
    "    \n",
    "    Key improvements:\n",
    "    - Bidirectional LSTM for context from both directions\n",
    "    - No pretrained embeddings (learns from scratch)\n",
    "    - Larger hidden dimension\n",
    "    - Proper projection of bidirectional states\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Learnable embeddings (no GloVe)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Project bidirectional hidden states to decoder dimension\n",
    "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_cell = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding_dropout(self.embedding(x))\n",
    "        \n",
    "        # BiLSTM outputs\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        outputs = self.lstm_dropout(outputs)\n",
    "        \n",
    "        # Combine forward and backward states\n",
    "        # hidden: (2, batch, hidden_dim) -> (batch, hidden_dim*2)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1)\n",
    "        \n",
    "        # Project to decoder dimension\n",
    "        hidden = torch.tanh(self.fc_hidden(hidden)).unsqueeze(0)\n",
    "        cell = torch.tanh(self.fc_cell(cell)).unsqueeze(0)\n",
    "        \n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Standard unidirectional decoder\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        embedded = self.embedding_dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        outputs = self.lstm_dropout(outputs)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Bahdanau attention mechanism\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Note: encoder outputs are bidirectional (hidden_dim*2)\n",
    "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)  # *3 because encoder is bidirectional\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (1, batch, hidden_dim)\n",
    "        # encoder_outputs: (batch, src_len, hidden_dim*2) from BiLSTM\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat hidden for each source position\n",
    "        hidden = hidden.permute(1, 0, 2).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Compute attention energies\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"Decoder with attention mechanism\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, attention, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # LSTM input: embedding + context (encoder is bidirectional)\n",
    "        self.lstm = nn.LSTM(embed_dim + hidden_dim * 2, hidden_dim, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding_dropout(self.embedding(x))\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Attention\n",
    "            attn_weights = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    "            context = torch.bmm(attn_weights, encoder_outputs)\n",
    "            \n",
    "            # Concatenate embedding and context\n",
    "            rnn_input = torch.cat((embedded[:, t:t+1, :], context), dim=2)\n",
    "            \n",
    "            # LSTM step\n",
    "            output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "            output = self.lstm_dropout(output)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Basic Seq2Seq model\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        _, hidden, cell = self.encoder(src)\n",
    "        predictions, _, _ = self.decoder(trg, hidden, cell)\n",
    "        return predictions\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    \"\"\"Seq2Seq with attention\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        enc_outputs, hidden, cell = self.encoder(src)\n",
    "        predictions, _, _ = self.decoder(trg, hidden, cell, enc_outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb386460",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 7. TRAINING FUNCTIONS\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264ebba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, clip=1.0):\n",
    "    \"\"\"Train one epoch\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for enc_inp, dec_inp, dec_tgt in dataloader:\n",
    "        enc_inp = enc_inp.to(device)\n",
    "        dec_inp = dec_inp.to(device)\n",
    "        dec_tgt = dec_tgt.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(enc_inp, dec_inp)\n",
    "        \n",
    "        # Calculate loss\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        dec_tgt = dec_tgt.reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, dec_tgt)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        non_pad = (dec_tgt != es_vocab['<pad>']).sum()\n",
    "        correct = ((output.argmax(1) == dec_tgt) & (dec_tgt != es_vocab['<pad>'])).sum()\n",
    "        acc = correct.float() / non_pad.float()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for enc_inp, dec_inp, dec_tgt in dataloader:\n",
    "            enc_inp = enc_inp.to(device)\n",
    "            dec_inp = dec_inp.to(device)\n",
    "            dec_tgt = dec_tgt.to(device)\n",
    "            \n",
    "            output = model(enc_inp, dec_inp)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            dec_tgt = dec_tgt.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, dec_tgt)\n",
    "            \n",
    "            non_pad = (dec_tgt != es_vocab['<pad>']).sum()\n",
    "            correct = ((output.argmax(1) == dec_tgt) & (dec_tgt != es_vocab['<pad>'])).sum()\n",
    "            acc = correct.float() / non_pad.float()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    \"\"\"Calculate perplexity from loss\"\"\"\n",
    "    return np.exp(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47fa99",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 8. INFERENCE FUNCTIONS\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ba5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_sentence, max_len=50):\n",
    "    \"\"\"Greedy decoding (always pick most probable token)\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_indices = text_to_indices(src_sentence, en_vocab, language='en')\n",
    "        src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "        \n",
    "        if isinstance(model, Seq2SeqWithAttention):\n",
    "            enc_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "        else:\n",
    "            _, hidden, cell = model.encoder(src_tensor)\n",
    "            enc_outputs = None\n",
    "        \n",
    "        trg_indices = [es_vocab['<sos>']]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            trg_tensor = torch.LongTensor([trg_indices[-1]]).unsqueeze(0).to(device)\n",
    "            \n",
    "            if isinstance(model, Seq2SeqWithAttention):\n",
    "                output, hidden, cell = model.decoder(trg_tensor, hidden, cell, enc_outputs)\n",
    "            else:\n",
    "                output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            \n",
    "            pred_token = output.argmax(2).item()\n",
    "            trg_indices.append(pred_token)\n",
    "            \n",
    "            if pred_token == es_vocab['<eos>']:\n",
    "                break\n",
    "        \n",
    "        trg_tokens = [es_vocab.get_itos()[idx] for idx in trg_indices[1:-1]]\n",
    "        return ' '.join(trg_tokens)\n",
    "\n",
    "def beam_search_decode(model, src_sentence, beam_width=5, max_len=50):\n",
    "    \"\"\"Beam search decoding\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_indices = text_to_indices(src_sentence, en_vocab, language='en')\n",
    "        src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "        \n",
    "        if isinstance(model, Seq2SeqWithAttention):\n",
    "            enc_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "        else:\n",
    "            _, hidden, cell = model.encoder(src_tensor)\n",
    "            enc_outputs = None\n",
    "        \n",
    "        beams = [([es_vocab['<sos>']], 0.0, hidden, cell)]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            \n",
    "            for seq, score, h, c in beams:\n",
    "                if seq[-1] == es_vocab['<eos>']:\n",
    "                    all_candidates.append((seq, score, h, c))\n",
    "                    continue\n",
    "                \n",
    "                trg_tensor = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)\n",
    "                \n",
    "                if isinstance(model, Seq2SeqWithAttention):\n",
    "                    output, new_h, new_c = model.decoder(trg_tensor, h, c, enc_outputs)\n",
    "                else:\n",
    "                    output, new_h, new_c = model.decoder(trg_tensor, h, c)\n",
    "                \n",
    "                log_probs = torch.log_softmax(output[0, 0], dim=0)\n",
    "                top_k_probs, top_k_indices = torch.topk(log_probs, beam_width)\n",
    "                \n",
    "                for prob, idx in zip(top_k_probs, top_k_indices):\n",
    "                    new_seq = seq + [idx.item()]\n",
    "                    new_score = score + prob.item()\n",
    "                    all_candidates.append((new_seq, new_score, new_h, new_c))\n",
    "            \n",
    "            beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            \n",
    "            if all(seq[-1] == es_vocab['<eos>'] for seq, _, _, _ in beams):\n",
    "                break\n",
    "        \n",
    "        best_seq = beams[0][0]\n",
    "        trg_tokens = [es_vocab.get_itos()[idx] for idx in best_seq[1:-1]]\n",
    "        return ' '.join(trg_tokens)\n",
    "\n",
    "def quick_test(model, test_phrases):\n",
    "    \"\"\"Quick test on common phrases\"\"\"\n",
    "    model.eval()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUICK TRANSLATION TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for phrase in test_phrases:\n",
    "        greedy = greedy_decode(model, phrase)\n",
    "        beam = beam_search_decode(model, phrase, beam_width=5)\n",
    "        print(f\"\\nEN: {phrase}\")\n",
    "        print(f\"Greedy: {greedy}\")\n",
    "        print(f\"Beam:   {beam}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e8a39",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 9. MODEL INITIALIZATION AND TRAINING\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16096487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING MODELS\n",
      "================================================================================\n",
      "\n",
      "Basic Model Parameters: 4,013,448\n",
      "Attention Model Parameters: 4,734,856\n",
      "\n",
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIALIZING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create models\n",
    "encoder = ImprovedEncoder(len(en_vocab), EMBED_DIM, HIDDEN_DIM, dropout=DROPOUT)\n",
    "decoder = Decoder(len(es_vocab), EMBED_DIM, HIDDEN_DIM, dropout=DROPOUT)\n",
    "model_basic = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "encoder_attn = ImprovedEncoder(len(en_vocab), EMBED_DIM, HIDDEN_DIM, dropout=DROPOUT)\n",
    "attention = Attention(HIDDEN_DIM)\n",
    "decoder_attn = DecoderWithAttention(len(es_vocab), EMBED_DIM, HIDDEN_DIM, attention, dropout=DROPOUT)\n",
    "model_attention = Seq2SeqWithAttention(encoder_attn, decoder_attn).to(device)\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=es_vocab['<pad>'])\n",
    "\n",
    "optimizer_basic = optim.Adam(model_basic.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_attn = optim.Adam(model_attention.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "if USE_LR_SCHEDULER:\n",
    "    scheduler_basic = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_basic, mode='min', factor=LR_SCHEDULER_FACTOR,\n",
    "        patience=LR_SCHEDULER_PATIENCE, min_lr=LR_SCHEDULER_MIN_LR\n",
    "    )\n",
    "    scheduler_attn = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_attn, mode='min', factor=LR_SCHEDULER_FACTOR,\n",
    "        patience=LR_SCHEDULER_PATIENCE, min_lr=LR_SCHEDULER_MIN_LR\n",
    "    )\n",
    "\n",
    "print(f\"\\nBasic Model Parameters: {sum(p.numel() for p in model_basic.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Attention Model Parameters: {sum(p.numel() for p in model_attention.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Test phrases for monitoring\n",
    "test_phrases = [\n",
    "    \"Hello\",\n",
    "    \"Good morning\",\n",
    "    \"I love you\",\n",
    "    \"Thank you\",\n",
    "    \"Where are you?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d84f2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  Train Loss: 4.7146 | Train Acc: 0.2816 | PPL: 111.56\n",
      "  Val Loss: 3.8072 | Val Acc: 0.3803 | PPL: 45.03\n",
      "Epoch 2/25\n",
      "  Train Loss: 3.4109 | Train Acc: 0.4197 | PPL: 30.29\n",
      "  Val Loss: 3.0641 | Val Acc: 0.4651 | PPL: 21.42\n",
      "Epoch 3/25\n",
      "  Train Loss: 2.7846 | Train Acc: 0.4878 | PPL: 16.19\n",
      "  Val Loss: 2.6429 | Val Acc: 0.5145 | PPL: 14.05\n",
      "Epoch 4/25\n",
      "  Train Loss: 2.3645 | Train Acc: 0.5373 | PPL: 10.64\n",
      "  Val Loss: 2.3581 | Val Acc: 0.5515 | PPL: 10.57\n",
      "Epoch 5/25\n",
      "  Train Loss: 2.0475 | Train Acc: 0.5791 | PPL: 7.75\n",
      "  Val Loss: 2.1652 | Val Acc: 0.5795 | PPL: 8.72\n",
      "\n",
      "================================================================================\n",
      "QUICK TRANSLATION TEST\n",
      "================================================================================\n",
      "\n",
      "EN: Hello\n",
      "Greedy: se <unk>\n",
      "Beam:   se <unk>\n",
      "\n",
      "EN: Good morning\n",
      "Greedy: buenas <unk>\n",
      "Beam:   buenas <unk>\n",
      "\n",
      "EN: I love you\n",
      "Greedy: te quiero que te <unk>\n",
      "Beam:   te quiero que <unk>\n",
      "\n",
      "EN: Thank you\n",
      "Greedy: gracias que te <unk>\n",
      "Beam:   <unk> algo?\n",
      "\n",
      "EN: Where are you?\n",
      "Greedy: ¿dónde están <unk>\n",
      "Beam:   ¿dónde están <unk>\n",
      "================================================================================\n",
      "Epoch 6/25\n",
      "  Train Loss: 1.8035 | Train Acc: 0.6149 | PPL: 6.07\n",
      "  Val Loss: 2.0257 | Val Acc: 0.5982 | PPL: 7.58\n",
      "Epoch 7/25\n",
      "  Train Loss: 1.6095 | Train Acc: 0.6445 | PPL: 5.00\n",
      "  Val Loss: 1.9649 | Val Acc: 0.6106 | PPL: 7.13\n",
      "Epoch 8/25\n",
      "  Train Loss: 1.4587 | Train Acc: 0.6699 | PPL: 4.30\n",
      "  Val Loss: 1.8735 | Val Acc: 0.6251 | PPL: 6.51\n",
      "Epoch 9/25\n",
      "  Train Loss: 1.3357 | Train Acc: 0.6901 | PPL: 3.80\n",
      "  Val Loss: 1.8204 | Val Acc: 0.6371 | PPL: 6.17\n",
      "Epoch 10/25\n",
      "  Train Loss: 1.2327 | Train Acc: 0.7079 | PPL: 3.43\n",
      "  Val Loss: 1.7898 | Val Acc: 0.6421 | PPL: 5.99\n",
      "\n",
      "================================================================================\n",
      "QUICK TRANSLATION TEST\n",
      "================================================================================\n",
      "\n",
      "EN: Hello\n",
      "Greedy: se <unk> a las <unk>\n",
      "Beam:   se <unk>\n",
      "\n",
      "EN: Good morning\n",
      "Greedy: lo <unk>\n",
      "Beam:   su <unk>\n",
      "\n",
      "EN: I love you\n",
      "Greedy: te quiero que te <unk>\n",
      "Beam:   te quiero <unk>\n",
      "\n",
      "EN: Thank you\n",
      "Greedy: gracias por lo que te <unk>\n",
      "Beam:   gracias <unk>\n",
      "\n",
      "EN: Where are you?\n",
      "Greedy: ¿dónde están <unk>\n",
      "Beam:   ¿dónde están <unk>\n",
      "================================================================================\n",
      "Epoch 11/25\n",
      "  Train Loss: 1.1455 | Train Acc: 0.7247 | PPL: 3.14\n",
      "  Val Loss: 1.7725 | Val Acc: 0.6472 | PPL: 5.89\n",
      "Epoch 12/25\n",
      "  Train Loss: 1.0696 | Train Acc: 0.7380 | PPL: 2.91\n",
      "  Val Loss: 1.7673 | Val Acc: 0.6486 | PPL: 5.86\n",
      "Epoch 13/25\n",
      "  Train Loss: 1.0054 | Train Acc: 0.7488 | PPL: 2.73\n",
      "  Val Loss: 1.7573 | Val Acc: 0.6514 | PPL: 5.80\n",
      "Epoch 14/25\n",
      "  Train Loss: 0.9457 | Train Acc: 0.7606 | PPL: 2.57\n",
      "  Val Loss: 1.7576 | Val Acc: 0.6530 | PPL: 5.80\n",
      "Epoch 15/25\n",
      "  Train Loss: 0.8910 | Train Acc: 0.7714 | PPL: 2.44\n",
      "  Val Loss: 1.7565 | Val Acc: 0.6555 | PPL: 5.79\n",
      "\n",
      "================================================================================\n",
      "QUICK TRANSLATION TEST\n",
      "================================================================================\n",
      "\n",
      "EN: Hello\n",
      "Greedy: <unk> a la <unk>\n",
      "Beam:   <unk> la <unk>\n",
      "\n",
      "EN: Good morning\n",
      "Greedy: se robó la boca al respecto.\n",
      "Beam:   su <unk>\n",
      "\n",
      "EN: I love you\n",
      "Greedy: te quiero que te guste.\n",
      "Beam:   te quiero que te guste.\n",
      "\n",
      "EN: Thank you\n",
      "Greedy: gracias por lo que te <unk>\n",
      "Beam:   gracias por <unk>\n",
      "\n",
      "EN: Where are you?\n",
      "Greedy: ¿dónde <unk>\n",
      "Beam:   ¿dónde <unk>\n",
      "================================================================================\n",
      "Epoch 16/25\n",
      "  Train Loss: 0.8416 | Train Acc: 0.7811 | PPL: 2.32\n",
      "  Val Loss: 1.7693 | Val Acc: 0.6555 | PPL: 5.87\n",
      "Epoch 17/25\n",
      "  Train Loss: 0.7962 | Train Acc: 0.7907 | PPL: 2.22\n",
      "  Val Loss: 1.7745 | Val Acc: 0.6568 | PPL: 5.90\n",
      "Epoch 18/25\n",
      "  Train Loss: 0.7576 | Train Acc: 0.7981 | PPL: 2.13\n",
      "  Val Loss: 1.7803 | Val Acc: 0.6585 | PPL: 5.93\n",
      "Epoch 19/25\n",
      "  Train Loss: 0.7194 | Train Acc: 0.8077 | PPL: 2.05\n",
      "  Val Loss: 1.7880 | Val Acc: 0.6591 | PPL: 5.98\n",
      "Epoch 20/25\n",
      "  Train Loss: 0.6278 | Train Acc: 0.8295 | PPL: 1.87\n",
      "  Val Loss: 1.7781 | Val Acc: 0.6642 | PPL: 5.92\n",
      "\n",
      "================================================================================\n",
      "QUICK TRANSLATION TEST\n",
      "================================================================================\n",
      "\n",
      "EN: Hello\n",
      "Greedy: les <unk> a la <unk>\n",
      "Beam:   <unk> la <unk>\n",
      "\n",
      "EN: Good morning\n",
      "Greedy: se fue bueno para la mañana.\n",
      "Beam:   se volvió bien la mañana.\n",
      "\n",
      "EN: I love you\n",
      "Greedy: te quiero <unk>\n",
      "Beam:   te quiero <unk>\n",
      "\n",
      "EN: Thank you\n",
      "Greedy: gracias a ti <unk>\n",
      "Beam:   tal vez <unk>\n",
      "\n",
      "EN: Where are you?\n",
      "Greedy: ¿dónde <unk>\n",
      "Beam:   ¿dónde <unk>\n",
      "================================================================================\n",
      "Epoch 21/25\n",
      "  Train Loss: 0.5942 | Train Acc: 0.8376 | PPL: 1.81\n",
      "  Val Loss: 1.7979 | Val Acc: 0.6632 | PPL: 6.04\n",
      "Epoch 22/25\n",
      "  Train Loss: 0.5710 | Train Acc: 0.8433 | PPL: 1.77\n",
      "  Val Loss: 1.8044 | Val Acc: 0.6628 | PPL: 6.08\n",
      "Epoch 23/25\n",
      "  Train Loss: 0.5509 | Train Acc: 0.8477 | PPL: 1.73\n",
      "  Val Loss: 1.8101 | Val Acc: 0.6641 | PPL: 6.11\n",
      "Epoch 24/25\n",
      "  Train Loss: 0.5089 | Train Acc: 0.8606 | PPL: 1.66\n",
      "  Val Loss: 1.8119 | Val Acc: 0.6662 | PPL: 6.12\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop example:\n",
    "history_attention = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model_attention, train_loader, optimizer_attn, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model_attention, val_loader, criterion, device)\n",
    "    \n",
    "    if USE_LR_SCHEDULER:\n",
    "        scheduler_attn.step(val_loss)\n",
    "    \n",
    "    history_attention['train_loss'].append(train_loss)\n",
    "    history_attention['train_acc'].append(train_acc)\n",
    "    history_attention['val_loss'].append(val_loss)\n",
    "    history_attention['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | PPL: {calculate_perplexity(train_loss):.2f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | PPL: {calculate_perplexity(val_loss):.2f}\")\n",
    "    \n",
    "    # Quick test every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        quick_test(model_attention, test_phrases)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CEIA-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
