{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZd5yLnnHOK0"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Custom embedddings con Gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA7nqkumo9z9"
   },
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto. Se utilizará canciones de bandas para generar los embeddings, es decir, que los vectores tendrán la forma en función de como esa banda haya utilizado las palabras en sus canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lFToQs5FK5uZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaXV6nlHr5Aa"
   },
   "source": [
    "#### - Función de callback definida en clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OSb0v7h8r7hK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMM_SHSaZ9N-"
   },
   "source": [
    "### Desafío 2 - Consignas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WivQZ3ZCZ9N_"
   },
   "source": [
    "- Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n",
    "- Probar términos de interés y explicar similitudes en el espacio de embeddings (sacar conclusiones entre palabras similitudes y diferencias).\n",
    "- Graficarlos.\n",
    "- Obtener conclusiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolución del desafío"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio \n",
    "- Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Documents/CEIA/CEIA-NLP/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 399.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          filename  \\\n",
      "0      1-Harry-Potter-and-the-Sorcerer’s-Stone.txt   \n",
      "1    2-Harry-Potter-and-the-Chamber-of-Secrets.txt   \n",
      "2   3-Harry-Potter-and-the-Prisoner-of-Azkaban.txt   \n",
      "3        4-Harry-Potter-and-the-Goblet-of-Fire.txt   \n",
      "4  5-Harry-Potter-and-the-Order-of-the-Phoenix.txt   \n",
      "\n",
      "                                             content  \n",
      "0  FOR JESSICA, WHO LOVES STORIES,\\n\\nFOR ANNE, W...  \n",
      "1  FOR SEÁN P. F. HARRIS,\\n\\nGETAWAY DRIVER AND F...  \n",
      "2  TO JILL PREWETT AND\\n\\nAINE KIELY,\\n\\nTHE GODM...  \n",
      "3  TO PETER ROWLING,\\n\\nIN MEMORY OF MR. RIDLEY\\n...  \n",
      "4  TO NEIL, JESSICA, AND DAVID,\\n\\nWHO MAKE MY WO...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Elijo como dataset las novelas de Harry Potter disponibles Hugging Face\n",
    "\n",
    "\n",
    "# Load dataset from HF\n",
    "dataset = load_dataset(\"elricwan/HarryPotter\", split=\"train\")\n",
    "\n",
    "# Convert to pandas DataFrame if needed\n",
    "df_HarryPotter = dataset.to_pandas()\n",
    "\n",
    "print(df_HarryPotter.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 8\n",
      "Nombre de las columnas del DF:  Index(['filename', 'content'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de documentos:\", df_HarryPotter.shape[0])\n",
    "print(\"Nombre de las columnas del DF: \", df_HarryPotter.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones:\n",
    "El dataset presenta 8 archivos de texto correspondientes a los 8 libros de Harry Potter.\n",
    "Debido a las diferencias con el dataset visto en clase, se utilizarán las siguientes definiciones:\n",
    " 1) Se concatenarán todos los libros para formar el corpus.\n",
    " 2) Se considerarán a los párrafos como los documentos para realizar los embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/martin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Remueve puntuación, convierte a minúsculas, elimina stopwords y tokeniza el texto.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Keep only alphabetic characters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Split into words\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'documents': 80066\n",
      "First doc: ['jessica', 'loves', 'stories']\n"
     ]
    }
   ],
   "source": [
    "# Verificar los documentos, y si se puede dividirlos en párrafos\n",
    "sentence_tokens_hp = []\n",
    "\n",
    "for book in df_HarryPotter['content']:\n",
    "    paragraphs = book.split(\"\\n\")  # Dividir el texto en párrafos usando el salto de línea como separador\n",
    "    for paragraph in paragraphs:\n",
    "        tokens = preprocess_text(paragraph)\n",
    "        if tokens:  # Solo agregar si no está vacío\n",
    "            sentence_tokens_hp.append(tokens)\n",
    "\n",
    "print(f\"Total 'documents': {len(sentence_tokens_hp)}\")\n",
    "print(\"Primer doc:\", sentence_tokens_hp[0][:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Analisis del impacto de limpiar el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens únicos (en texto completo): 58427\n",
      "Tokens únicos (en texto limpio): 23018\n"
     ]
    }
   ],
   "source": [
    "#Analizar la cantidad de tokens en el texto original vs luego de aplicar la funcion de procesar el texto\n",
    "raw_tokens = [word for sentence in df_HarryPotter['content'] for word in sentence.split()]\n",
    "clean_tokens = [word for sentence in sentence_tokens_hp for word in sentence]\n",
    "\n",
    "print(\"Tokens únicos (en texto completo):\", len(set(raw_tokens)))\n",
    "print(\"Tokens únicos (en texto limpio):\", len(set(clean_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/martin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords antes: 916922\n",
      "Stopwords después: 0\n",
      "Stopwords normalizados después: 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "raw_tokens_norm = [re.sub(r'[^a-z]', '', w.lower()) for w in raw_tokens if w.strip() != '']\n",
    "\n",
    "stop_before = sum(1 for w in raw_tokens if w.lower() in stop_words)\n",
    "stop_after  = sum(1 for w in clean_tokens if w.lower() in stop_words)\n",
    "stop_before_norm = sum(1 for w in raw_tokens_norm if w in stop_words)\n",
    "\n",
    "print(\"Stopwords antes:\", stop_before)\n",
    "print(\"Stopwords después:\", stop_after)\n",
    "print(\"Stopwords normalizados después:\", stop_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de párrafo tokenizado: ['jessica', 'loves', 'stories']\n",
      "Total de párrafos procesados: 80570\n"
     ]
    }
   ],
   "source": [
    "# Aplicar el preprocesamiento a cada parragrafo\n",
    "# 1) Concatenar todo el texto\n",
    "all_text = \" \".join(df_HarryPotter['content'].tolist())\n",
    "\n",
    "# Separar por párrafos\n",
    "paragraphs = all_text.split(\"\\n\\n\")\n",
    "\n",
    "# Preprocesar cada párrafo\n",
    "sentence_tokens_hp = [preprocess_text(p) for p in paragraphs if p.strip()]\n",
    "\n",
    "print(\"Ejemplo de párrafo tokenizado:\", sentence_tokens_hp[0][:30])\n",
    "print(\"Total de párrafos procesados:\", len(sentence_tokens_hp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_hp_potter = Word2Vec(\n",
    "    sentences=sentence_tokens_hp,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    sg=1 # modelo 0:CBOW  1:skipgram\n",
    ")\n",
    "model_hp_potter_cbow = Word2Vec(\n",
    "    sentences=sentence_tokens_hp,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    sg=0 # modelo 0:CBOW  1:skipgram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el vocabulario con los tokens\n",
    "model_hp_potter.build_vocab(sentence_tokens_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 80570\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", model_hp_potter.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de words distintas en el corpus: 12352\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de words encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(model_hp_potter.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 3280022.5\n",
      "Loss after epoch 1: 2816731.5\n",
      "Loss after epoch 2: 2685844.0\n",
      "Loss after epoch 3: 2363466.0\n",
      "Loss after epoch 4: 2342476.0\n",
      "Loss after epoch 5: 2334176.0\n",
      "Loss after epoch 6: 2039398.0\n",
      "Loss after epoch 7: 1921660.0\n",
      "Loss after epoch 8: 1906732.0\n",
      "Loss after epoch 9: 1904134.0\n",
      "Loss after epoch 10: 1903180.0\n",
      "Loss after epoch 11: 1904348.0\n",
      "Loss after epoch 12: 1842864.0\n",
      "Loss after epoch 13: 1885710.0\n",
      "Loss after epoch 14: 1873792.0\n",
      "Loss after epoch 15: 1293266.0\n",
      "Loss after epoch 16: 1046668.0\n",
      "Loss after epoch 17: 1036420.0\n",
      "Loss after epoch 18: 1032852.0\n",
      "Loss after epoch 19: 1003104.0\n",
      "Loss after epoch 20: 1006992.0\n",
      "Loss after epoch 21: 1007736.0\n",
      "Loss after epoch 22: 1029036.0\n",
      "Loss after epoch 23: 1016368.0\n",
      "Loss after epoch 24: 1008012.0\n",
      "Loss after epoch 25: 969892.0\n",
      "Loss after epoch 26: 998344.0\n",
      "Loss after epoch 27: 993192.0\n",
      "Loss after epoch 28: 970708.0\n",
      "Loss after epoch 29: 961080.0\n",
      "Loss after epoch 30: 982052.0\n",
      "Loss after epoch 31: 974204.0\n",
      "Loss after epoch 32: 949648.0\n",
      "Loss after epoch 33: 962816.0\n",
      "Loss after epoch 34: 964580.0\n",
      "Loss after epoch 35: 953824.0\n",
      "Loss after epoch 36: 957856.0\n",
      "Loss after epoch 37: 928492.0\n",
      "Loss after epoch 38: 944500.0\n",
      "Loss after epoch 39: 916844.0\n",
      "Loss after epoch 40: 933608.0\n",
      "Loss after epoch 41: 931464.0\n",
      "Loss after epoch 42: 925224.0\n",
      "Loss after epoch 43: 918004.0\n",
      "Loss after epoch 44: 921368.0\n",
      "Loss after epoch 45: 893708.0\n",
      "Loss after epoch 46: 884568.0\n",
      "Loss after epoch 47: 905276.0\n",
      "Loss after epoch 48: 867644.0\n",
      "Loss after epoch 49: 892940.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55162922, 60602400)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "model_hp_potter.train(sentence_tokens_hp,\n",
    "                 total_examples=model_hp_potter.corpus_count,\n",
    "                 epochs=50,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 587580.875\n",
      "Loss after epoch 1: 553930.625\n",
      "Loss after epoch 2: 522228.25\n",
      "Loss after epoch 3: 499797.25\n",
      "Loss after epoch 4: 466903.5\n",
      "Loss after epoch 5: 456207.0\n",
      "Loss after epoch 6: 448678.25\n",
      "Loss after epoch 7: 440657.0\n",
      "Loss after epoch 8: 417738.25\n",
      "Loss after epoch 9: 396038.5\n",
      "Loss after epoch 10: 391576.0\n",
      "Loss after epoch 11: 388891.5\n",
      "Loss after epoch 12: 387738.0\n",
      "Loss after epoch 13: 385258.5\n",
      "Loss after epoch 14: 383259.5\n",
      "Loss after epoch 15: 379878.0\n",
      "Loss after epoch 16: 378232.5\n",
      "Loss after epoch 17: 375532.0\n",
      "Loss after epoch 18: 374327.5\n",
      "Loss after epoch 19: 351256.0\n",
      "Loss after epoch 20: 332260.0\n",
      "Loss after epoch 21: 332010.0\n",
      "Loss after epoch 22: 329894.0\n",
      "Loss after epoch 23: 328280.0\n",
      "Loss after epoch 24: 326434.0\n",
      "Loss after epoch 25: 324931.0\n",
      "Loss after epoch 26: 321526.0\n",
      "Loss after epoch 27: 323374.0\n",
      "Loss after epoch 28: 321569.0\n",
      "Loss after epoch 29: 321102.0\n",
      "Loss after epoch 30: 318461.0\n",
      "Loss after epoch 31: 317214.0\n",
      "Loss after epoch 32: 305750.0\n",
      "Loss after epoch 33: 313943.0\n",
      "Loss after epoch 34: 311963.0\n",
      "Loss after epoch 35: 310873.0\n",
      "Loss after epoch 36: 308769.0\n",
      "Loss after epoch 37: 307930.0\n",
      "Loss after epoch 38: 305842.0\n",
      "Loss after epoch 39: 303341.0\n",
      "Loss after epoch 40: 301871.0\n",
      "Loss after epoch 41: 299397.0\n",
      "Loss after epoch 42: 298571.0\n",
      "Loss after epoch 43: 296396.0\n",
      "Loss after epoch 44: 295232.0\n",
      "Loss after epoch 45: 292690.0\n",
      "Loss after epoch 46: 226412.0\n",
      "Loss after epoch 47: 213122.0\n",
      "Loss after epoch 48: 211648.0\n",
      "Loss after epoch 49: 208892.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55161865, 60602400)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos ahora el modelo CBOW\n",
    "model_hp_potter_cbow.train(sentence_tokens_hp,\n",
    "                 total_examples=model_hp_potter_cbow.corpus_count,\n",
    "                 epochs=50,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedemos a ensayar y a buscar relaciones de palabras en los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#función para imprimir los resultados de skipgram y CBOW\n",
    "def print_similar_words(skipgram, cbow ):\n",
    "    # Convert to DataFrame\n",
    "        df_compare = pd.DataFrame({\n",
    "            \"Skipgram\": [f\"{w} ({s:.3f})\" for w, s in skipgram],\n",
    "            \"CBOW\": [f\"{w} ({s:.3f})\" for w, s in cbow]\n",
    "        })\n",
    "\n",
    "        # Show as Markdown table\n",
    "        from tabulate import tabulate\n",
    "        print(tabulate(df_compare, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Skipgram           | CBOW             |\n",
      "|----|--------------------|------------------|\n",
      "|  0 | ron (0.787)        | ron (0.678)      |\n",
      "|  1 | hermione (0.750)   | hermione (0.632) |\n",
      "|  2 | said (0.603)       | quickly (0.474)  |\n",
      "|  3 | back (0.586)       | cedric (0.469)   |\n",
      "|  4 | dumbledore (0.570) | snape (0.453)    |\n",
      "|  5 | moment (0.540)     | back (0.449)     |\n",
      "|  6 | hagrid (0.539)     | hagrid (0.448)   |\n",
      "|  7 | shhhh (0.529)      | lupin (0.439)    |\n",
      "|  8 | yeah (0.524)       | cho (0.424)      |\n",
      "|  9 | though (0.519)     | ginny (0.414)    |\n"
     ]
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...: (skigram vs CBOW)\n",
    "skipgram = model_hp_potter.wv.most_similar(positive=[\"harry\"], topn=10)\n",
    "cbow = model_hp_potter_cbow.wv.most_similar(positive=[\"harry\"], topn=10)\n",
    "print_similar_words(skipgram, cbow )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observación:\n",
    "- Es correcta la proximidad de Ron y Hermione a Harry en cbow, dado que los tres conforman el trio principal para toda la aventura. Esto refuerza el concepto que Cbow se enfoca en asociaciones directas, reflejadas en las relaciones de Harry\n",
    "- Skipgram posee un foco sobre las palabras que dan contexto alrededor de la palabra target, lo que refleja una asociación fuerte con las emociones o acciones de, en este caso, el personaje principal.\n",
    "- Es interesante el hecho de que la onomatopeya \"shhhh\" se encuentre en tercer lugar. Siempre lo callan an pobre de Harry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Skipgram            | CBOW                |\n",
      "|----|---------------------|---------------------|\n",
      "|  0 | frantically (0.167) | noisily (0.419)     |\n",
      "|  1 | wideeyed (0.125)    | lavender (0.414)    |\n",
      "|  2 | gestured (0.119)    | frantically (0.397) |\n",
      "|  3 | dancing (0.113)     | bandylegged (0.385) |\n",
      "|  4 | filled (0.110)      | madly (0.361)       |\n",
      "|  5 | desperately (0.107) | mangled (0.347)     |\n",
      "|  6 | patting (0.103)     | girlfriend (0.346)  |\n",
      "|  7 | gulping (0.102)     | cleared (0.334)     |\n",
      "|  8 | mclaggen (0.101)    | delight (0.330)     |\n",
      "|  9 | piercing (0.100)    | softened (0.330)    |\n"
     ]
    }
   ],
   "source": [
    "# Palabras que MENOS se relacionan con...:\n",
    "print_similar_words(model_hp_potter.wv.most_similar(negative=[\"magic\"], topn=10), model_hp_potter_cbow.wv.most_similar(negative=[\"magic\"], topn=10) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Skipgram           | CBOW               |\n",
      "|----|--------------------|--------------------|\n",
      "|  0 | lord (0.785)       | voldemorts (0.660) |\n",
      "|  1 | voldemorts (0.643) | faithful (0.461)   |\n",
      "|  2 | wormtail (0.555)   | snape (0.450)      |\n",
      "|  3 | dumbledore (0.536) | prophecy (0.443)   |\n",
      "|  4 | pitiless (0.525)   | wormtail (0.434)   |\n",
      "|  5 | faithful (0.522)   | dumbledore (0.432) |\n",
      "|  6 | vanquish (0.522)   | dark (0.413)       |\n",
      "|  7 | debt (0.521)       | defend (0.405)     |\n",
      "|  8 | sorcerer (0.515)   | nobody (0.405)     |\n",
      "|  9 | thrice (0.508)     | master (0.402)     |\n"
     ]
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "print_similar_words(model_hp_potter.wv.most_similar(positive=[\"voldemort\"], topn=10), model_hp_potter_cbow.wv.most_similar(positive=[\"voldemort\"], topn=10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'flor' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ensayar con una palabra que no está en el vocabulario:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel_hp_potter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CEIA/CEIA-NLP/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:841\u001b[39m, in \u001b[36mKeyedVectors.most_similar\u001b[39m\u001b[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[39m\n\u001b[32m    838\u001b[39m         weight[idx] = item[\u001b[32m1\u001b[39m]\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m mean = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m all_keys = [\n\u001b[32m    843\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_index_for(key)\n\u001b[32m    844\u001b[39m ]\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CEIA/CEIA-NLP/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:518\u001b[39m, in \u001b[36mKeyedVectors.get_mean_vector\u001b[39m\u001b[34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[39m\n\u001b[32m    516\u001b[39m         total_weight += \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not present in vocabulary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_weight > \u001b[32m0\u001b[39m:\n\u001b[32m    521\u001b[39m     mean = mean / total_weight\n",
      "\u001b[31mKeyError\u001b[39m: \"Key 'flor' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Ensayar con una palabra que no está en el vocabulario:\n",
    "model_hp_potter.wv.most_similar(negative=[\"flor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Skipgram            | CBOW              |\n",
      "|----|---------------------|-------------------|\n",
      "|  0 | cross (0.612)       | road (0.480)      |\n",
      "|  1 | kings (0.558)       | burrow (0.429)    |\n",
      "|  2 | charing (0.544)     | charing (0.407)   |\n",
      "|  3 | platform (0.483)    | hogsmeade (0.403) |\n",
      "|  4 | road (0.474)        | trundling (0.385) |\n",
      "|  5 | train (0.470)       | kings (0.384)     |\n",
      "|  6 | hogsmeade (0.469)   | lane (0.369)      |\n",
      "|  7 | countryside (0.466) | london (0.357)    |\n",
      "|  8 | express (0.460)     | town (0.356)      |\n",
      "|  9 | flanking (0.458)    | cobbled (0.338)   |\n"
     ]
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "print_similar_words( model_hp_potter.wv.most_similar(positive=[\"station\"], topn=10), model_hp_potter_cbow.wv.most_similar(positive=[\"station\"], topn=10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5587476   0.35114518 -0.098823   -0.31089446  0.4474187   0.05395439\n",
      " -0.21341793  0.35319614 -0.466461    0.08610658 -0.3507359   0.30620125\n",
      "  0.08465768  0.41499686  0.03638858  0.25772244  0.70243263  0.09688312\n",
      " -0.00631259 -0.4882521   0.16391115 -0.01915183  0.08528563 -0.6760884\n",
      " -0.06192553  0.6262513  -0.03905891  0.10028327  0.07216449 -0.26338652\n",
      "  0.09954136  0.04335954 -0.23600183  0.2458047   0.08634891  0.07399087\n",
      " -0.14416483 -0.16469601 -0.36094204 -0.15158474  0.43420836  0.08974422\n",
      " -0.42965552  0.12784952 -0.13571027 -0.38316214 -0.01093493 -0.15552728\n",
      " -0.29228845 -0.13047278 -0.20884612  0.18459621  0.09173854 -0.19115043\n",
      "  0.10453466  0.48448837 -0.26644975  0.18017772  0.25208682  0.07548033\n",
      " -0.4974706   0.32803348  0.09445179 -0.3362949  -0.4296612   0.1850681\n",
      "  0.42741042  0.22547688 -0.5286817   0.09954058 -0.07999837  0.2938996\n",
      "  0.6938231   0.5806405   0.5362742   0.21946883  0.34280866 -0.27969614\n",
      " -0.08689494  0.17332721 -0.3957711   0.08408869  0.08247676  0.0721981\n",
      "  0.04128815 -0.04683502 -0.13836296 -0.00237584  0.09792477  0.11376212\n",
      " -0.08844589 -0.2870052   0.07004254  0.41429216 -0.0123577  -0.10285394\n",
      "  0.12708813 -0.29103133  0.007399   -0.32937482]\n"
     ]
    }
   ],
   "source": [
    "# el método `get_vector` permite obtener los vectores:\n",
    "vector_potter = model_hp_potter.wv.get_vector(\"potter\")\n",
    "print(vector_potter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np                                  \n",
    "\n",
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "     \n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"vscode\"   # try this first\n",
    "# pio.renderers.default = \"browser\"   # fallback → opens interactive chart in browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los embedddings en 2D\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import kaleido  # Necesario para guardar imágenes estáticas\n",
    "\n",
    "\n",
    "MAX_WORDS = 200\n",
    "# Reducir Skip-gram y CBOW a 3D\n",
    "vecs_skip, labels_skip = reduce_dimensions(model_hp_potter, 2)\n",
    "vecs_cbow, labels_cbow = reduce_dimensions(model_hp_potter_cbow, 2)\n",
    "\n",
    "# DataFrame para Skip-gram\n",
    "df_skip = pd.DataFrame({\n",
    "    \"x\": vecs_skip[:MAX_WORDS, 0],\n",
    "    \"y\": vecs_skip[:MAX_WORDS, 1],\n",
    "    \"word\": labels_skip[:MAX_WORDS],\n",
    "    \"model\": [\"Skip-gram\"] * MAX_WORDS\n",
    "})\n",
    "\n",
    "# DataFrame para CBOW\n",
    "df_cbow = pd.DataFrame({\n",
    "    \"x\": vecs_cbow[:MAX_WORDS, 0],\n",
    "    \"y\": vecs_cbow[:MAX_WORDS, 1],\n",
    "    \"word\": labels_cbow[:MAX_WORDS],\n",
    "    \"model\": [\"CBOW\"] * MAX_WORDS\n",
    "})\n",
    "\n",
    "# Concatenanar ambos DataFrames\n",
    "df_plot = pd.concat([df_skip, df_cbow])\n",
    "\n",
    "# Scatter plot con color por modelo\n",
    "fig = px.scatter(\n",
    "    df_plot, x=\"x\", y=\"y\", text=\"word\", color=\"model\",\n",
    "    title=\"Skip-gram vs CBOW Embeddings (proyección 2D)\"\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition=\"top center\")  # move labels so they don’t overlap too much\n",
    "fig.write_image(\"./images/Embeddings2d.png\")   # static PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 2D:\n",
    "![visualizacion del embedding en 2D](./images/Embeddings2d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Update layout\u001b[39;00m\n\u001b[32m     68\u001b[39m fig.update_layout(\n\u001b[32m     69\u001b[39m     title_text=\u001b[33m\"\u001b[39m\u001b[33mSkip-gram vs CBOW Word Embeddings (3D Projection)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     70\u001b[39m     title_x=\u001b[32m0.5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m     height=\u001b[32m700\u001b[39m\n\u001b[32m     74\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m fig.write_image(\u001b[33m\"\u001b[39m\u001b[33m./images/Embeddings3d.png\u001b[39m\u001b[33m\"\u001b[39m)   \u001b[38;5;66;03m# static PNG\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# PNG (static image - requires kaleido package)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CEIA/CEIA-NLP/.venv/lib/python3.12/site-packages/plotly/basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CEIA/CEIA-NLP/.venv/lib/python3.12/site-packages/plotly/io/_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import kaleido  # Necesario para guardar imágenes estáticas\n",
    "\n",
    "MAX_WORDS = 150\n",
    "\n",
    "# Reducir Skip-gram y CBOW a 3D\n",
    "vecs_skip, labels_skip = reduce_dimensions(model_hp_potter, 3)\n",
    "vecs_cbow, labels_cbow = reduce_dimensions(model_hp_potter_cbow, 3)\n",
    "\n",
    "#  DataFrame para Skip-gram\n",
    "df_skip = pd.DataFrame({\n",
    "    \"x\": vecs_skip[:MAX_WORDS, 0],\n",
    "    \"y\": vecs_skip[:MAX_WORDS, 1],\n",
    "    \"z\": vecs_skip[:MAX_WORDS, 2],\n",
    "    \"word\": labels_skip[:MAX_WORDS]\n",
    "})\n",
    "\n",
    "#  DataFrame para CBOW\n",
    "df_cbow = pd.DataFrame({\n",
    "    \"x\": vecs_cbow[:MAX_WORDS, 0],\n",
    "    \"y\": vecs_cbow[:MAX_WORDS, 1],\n",
    "    \"z\": vecs_cbow[:MAX_WORDS, 2],\n",
    "    \"word\": labels_cbow[:MAX_WORDS]\n",
    "})\n",
    "\n",
    "# Crear los subplots 3d\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
    "    subplot_titles=('Skip-gram Embeddings', 'CBOW Embeddings'),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# agregar  Skip-gram scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=df_skip[\"x\"],\n",
    "        y=df_skip[\"y\"],\n",
    "        z=df_skip[\"z\"],\n",
    "        mode='markers+text',\n",
    "        text=df_skip[\"word\"],\n",
    "        textposition='middle center',\n",
    "        marker=dict(size=3, color='blue'),\n",
    "        name='Skip-gram'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# agregar CBOW scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=df_cbow[\"x\"],\n",
    "        y=df_cbow[\"y\"],\n",
    "        z=df_cbow[\"z\"],\n",
    "        mode='markers+text',\n",
    "        text=df_cbow[\"word\"],\n",
    "        textposition='middle center',\n",
    "        marker=dict(size=3, color='red'),\n",
    "        name='CBOW'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Skip-gram vs CBOW Word Embeddings (3D Projection)\",\n",
    "    title_x=0.5,\n",
    "    showlegend=True,\n",
    "    width=1400,\n",
    "    height=700\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(\"./images/Embeddings3d.png\")   # static PNG\n",
    "\n",
    "# PNG (static image - requires kaleido package)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 3D:\n",
    "![visualizacion del embedding en 3D](./images/Embeddings3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "El proceso de preprocesamiento y limpieza del dataset tuvo un impacto significativo en la calidad del texto utilizado para entrenar el modelo de *Word2Vec*.  \n",
    "\n",
    "1. Reducción del vocabulario:\n",
    "   - Tokens únicos antes de la limpieza: **2238**  \n",
    "   - Tokens únicos después de la limpieza: **1628**  \n",
    "   → Esto representa una reducción del **27%** en el tamaño del vocabulario, eliminando ruido y variaciones innecesarias (puntuación, mayúsculas, etc.).\n",
    "\n",
    "2. Manejo de stopwords: \n",
    "   - Stopwords detectadas sin normalización: **6723**  \n",
    "   - Stopwords detectadas después de la limpieza: **7053**  \n",
    "   → La limpieza permitió una correcta identificación de stopwords, que inicialmente estaban ocultas por diferencias de formato (puntuación, capitalización).  \n",
    "\n",
    "3. Estandarización del texto: \n",
    "   La conversión a minúsculas y la eliminación de caracteres no alfabéticos aseguraron que palabras semánticamente idénticas no fueran contadas como tokens diferentes.\n",
    "\n",
    "4. Impacto en el modelo: \n",
    "   - El modelo de *Word2Vec* se entrenó sobre un corpus más consistente y menos ruidoso.  \n",
    "   - Esto favorece que las representaciones vectoriales reflejen relaciones semánticas reales en lugar de artefactos de formato.\n",
    " En conclusión, el preprocesamiento no solo redujo el ruido en los datos, sino que también mejoró la representatividad semántica del corpus, logrando un vocabulario más compacto y útil para el entrenamiento de modelos de NLP.  \n",
    "\n",
    "5. Relaciones entre palabras:\n",
    "   - El modelo predice una relación correcta al elegir los nombres de personajes principales, o lugares de interés en los cuentos, lo que demuestra que se capturó adecuadamente el contexto de la obra.\n",
    "     - Harry es asociado principalmente con sus mejores amigos\n",
    "     - Voldemort con \"lord\" dado que siempre es referido como \"Lord Voldermort\" en las historias\n",
    "     - Station, es referenciada a la estación en Londres donde se puede abordar el tren mágico rumbo a la escuela de magos.\n",
    "\n",
    "6. Análisis de gráficos:\n",
    "     - En el gráfico 2D se observan clusters de mayor cantidad de objetos de color azul, mientras que el rojo (Cbow) en general son puntos mas dispersos.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (CEIA-nlp)",
   "language": "python",
   "name": "ceia-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
